import zmq
import json
import traceback
import sys, os
import pickle
import torch
import io
import time

import os, zmq, json, cv2, numpy as np, base64, datetime, traceback
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt

from scipy import signal
from PIL import Image

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


DEBUG_MODE = True
LOG_DIR = "logs"
PORT = 55552

M_cache = None

# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¸°ì¤€)
plt.rcParams['font.family'] = 'Malgun Gothic'
# âœ… ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False



# ==========================
# ğŸ”§ ì„¤ì •
# ==========================
class CFG:
    folder_path = "C:/dinov3"
    model_path = "C:/dinov3/ibakoreaSystem/segmentation/laser_segmentation/weights/polygon_classifier.pkl"
    test_dir = rf"C:/Users/ë ˆë…¸ë²„/Downloads/Backup_í˜„ì¥/Backup_20250716/TOP_20250716142904" # ë‹¨ì°¨ ì°¨ì´ ì¢€ ë‚˜ëŠ”ê±°

    # ì „ì²˜ë¦¬ (ì¤‘ê°„ ë¶€ë¶„ ì œê±°)
    REMOVE_RATIO = 0.2

    PATCH_SIZE = 8
    IMAGENET_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_STD = (0.229, 0.224, 0.225)
    WEIGHTS_PATH = "C:/dinov3/dinov3_vits16_pretrain_lvd1689m-08c60483.pth"

    MODEL_TO_NUM_LAYERS = {
        "dinov3_vits16": 12,
        "dinov3_vits16plus": 12,
        "dinov3_vitb16": 12,
        "dinov3_vitl16": 24,
        "dinov3_vith16plus": 32,
        "dinov3_vit7b16": 40,
    }
    MODEL_NAME = "dinov3_vits16"
    n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]
    IMAGE_SIZE = 768
    plot_result = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    # device = "cpu"

def write_log(message: str):
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    os.makedirs(LOG_DIR, exist_ok=True)
    log_path = os.path.join(LOG_DIR, f"{today}.log")

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{timestamp}] {message}"

    with open(log_path, "a", encoding="utf-8") as f:
        f.write(line + "\n")

    if DEBUG_MODE:
        print(line)


def decode_base64_image(img_b64):
    try:
        img_bytes = base64.b64decode(img_b64)
        img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
        return img
    except Exception as e:
        write_log(f"âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None


def model_load():
    with open(CFG.model_path, "rb") as f:
        clf = pickle.load(f)

    model = torch.hub.load(repo_or_dir=CFG.folder_path, model=CFG.MODEL_NAME, source="local", weights=CFG.WEIGHTS_PATH)
    use_fp16 = torch.cuda.is_available()
    model = model.eval().to(CFG.device)
    if use_fp16:
        model = model.half()
        print("ğŸš€ GPU + FP16 ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
    else:
        print("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (FP32)")

    try:
        dummy = torch.randn(1, 3, CFG.IMAGE_SIZE, CFG.IMAGE_SIZE, device=CFG.device)
        if use_fp16:
            dummy = dummy.half()
        with torch.inference_mode():
            _ = model(dummy)
        print("ğŸ”¥ ëª¨ë¸ warm-up ì™„ë£Œ")
    except Exception as e:
        print(f"âš ï¸ warm-up ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return model, clf, CFG.device, use_fp16



def main():
    model, clf, device, use_fp16 = model_load()
    context = zmq.Context()
    socket = context.socket(zmq.REP)
    socket.bind(f"tcp://*:{PORT}")

    print(f"âœ… ZeroMQ ì„œë²„ ëŒ€ê¸° ì¤‘... (tcp://*:{PORT})")

    while True:
        try:
            msg = socket.recv_string()
            req = json.loads(msg)

            cmd = req.get("cmd", "")
            rotate_angle = float(req.get("rotate_angle", 1.3))
            remove_ratio = float(req.get("remove_ratio", 0.15))

            # ë§Œì•½ ìš©ì ‘ì„ ì´ ê³ ì •ì´ë©´ ã…‡ã…‡ bottom, top ë‚˜ëˆ ì„œ ì§€ìš°ëŠ” ê³³ ì§€ì •í•˜ë©´ ë  ë“¯ í•˜ë„¤ìš”
            if cmd == "infer":
                print(f"ğŸ“© ìš”ì²­ ìˆ˜ì‹ : rotate={rotate_angle}") # ì¼ë‹¨ 1.3ìœ¼ë¡œ ê³ ì •

                img_b64 = req.get("image_data", None)
                if img_b64 is None:
                    raise ValueError("image_dataê°€ ì—†ìŠµë‹ˆë‹¤.")

                image_np = decode_base64_image(img_b64)
                if image_np is None:
                    raise ValueError("ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")

                # max_height_ratio -> í•˜ë‚˜ì˜ ì„ ë§Œ ê°ì§€ë˜ì—ˆì„ë•Œ original ì´ë¯¸ì§€ì˜ ëª‡í¼ì„¼íŠ¸ê¹Œì§€ ë„ë‹¬í•˜ë©´ ë™ì¼í•œ ì„ ìƒì— ë†“ì—¬ìˆë‹¤ê³  íŒë‹¨í• ê²ƒì¸ê°€
                # -> ì£¼ì˜ í•´ì•¼í•¨ ì˜¤íƒì§€ê°€ ë  ìˆ˜ ìˆìœ¼ë‹ˆ
                result = infer_image_one(model, clf, device, use_fp16, image_np, file_name="", rotate_angle=rotate_angle, remove_ratio=remove_ratio, max_height_ratio=0.7, plot_result=CFG.plot_result)

                if isinstance(result, tuple):
                    socket.send_string(json.dumps({
                        "status": "ê²€ì¶œ ì™„ë£Œ",
                        "upper_x": round(result[0]),
                        "lower_x": round(result[1])
                    }))
                else:
                    # ê²€ì¶œ ì‹¤íŒ¨ì‹œ -1, -1ì„ ì „ì†¡í•¨
                    socket.send_string(json.dumps({
                        "status": "ê²€ì¶œ ì‹¤íŒ¨",
                        "upper_x": result[0],
                        "lower_x": result[1]
                    }))

            else:
                socket.send_string(json.dumps({"status": "ERROR", "msg": "unknown command"}))

        except Exception as e:
            err_msg = f"{type(e).__name__}: {str(e)}"
            print("âŒ Error:", err_msg)
            traceback.print_exc()
            socket.send_string(json.dumps({"status": "ERROR", "msg": err_msg}))

# ==========================
# ğŸ”¹ ì´ë¯¸ì§€ í´ë” ì¶”ë¡  (ì§ì„  ê²€ì¶œ í¬í•¨)
# ==========================
def infer_image_one(model, clf, device, use_fp16, test_image: np.ndarray, file_name: str = "", rotate_angle: float = 1.3, remove_ratio: float = 0.15, max_height_ratio: float = 0.7, plot_result: bool = False):
    global M_cache

    start_time = time.time()
    
    w, h = test_image.size

    # ğŸ”¹ ì—†ì•¨ ë¹„ìœ¨ (ì˜ˆ: ê°€ìš´ë° 15%)
    y1 = int(h * (0.5 - remove_ratio / 2))
    y2 = int(h * (0.5 + remove_ratio / 2))

    # --- íšŒì „ ---
    img_np = np.array(test_image)
    img_np[y1:y2, :] = 0  # ì´ë¯¸ì§€ íšŒì „

    h, w = img_np.shape[:2]

    # if M_cache is None:
    #     center = (w // 2, h // 2)
    #     M_cache = cv2.getRotationMatrix2D(center, rotate_angle, 1.0)

    # ë‚˜ì¤‘ì— rotation ê³ ì •ì´ë©´ ìœ„ì—ê±¸ë¡œ í•˜ê³ , ê·¸ê±°ì•„ë‹ë• ì´ê±¸ë¡œ ì‚¬ìš©í•´ì•¼í•¨. -> ìºì‰¬ë•Œë¬¸ì— ë””ë²„ê¹…ì´ ì•ˆë˜ì„œ ì¼ë‹¨ ì´ë ‡ê²Œ ë³€ê²½
    center = (w // 2, h // 2)
    M_cache = cv2.getRotationMatrix2D(center, rotate_angle, 1.0)    
    rotated = cv2.warpAffine(img_np, M_cache, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

    test_image = Image.fromarray(rotated)

    # --- ì „ì²˜ë¦¬ ---
    test_image_resized = resize_transform(test_image)
    test_image_normalized = TF.normalize(test_image_resized,mean=CFG.IMAGENET_MEAN,std=CFG.IMAGENET_STD).unsqueeze(0).to(device)
    if use_fp16:
        test_image_normalized = test_image_normalized.half()

    # --- DINO íŠ¹ì§• ---
    with torch.inference_mode():
        feats = model.get_intermediate_layers(
            test_image_normalized, n=range(CFG.n_layers), reshape=True, norm=True
        )
        x = feats[-1].squeeze().detach().cpu()
        dim, H_feat, W_feat = x.shape
        x = x.view(dim, -1).permute(1, 0)

    # --- classifier ---
    fg_score = clf.predict_proba(x)[:, 1]
    fg_score = fg_score.reshape(H_feat, W_feat)
    fg_score_mf = signal.medfilt2d(fg_score, kernel_size=3)

    # --- ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œ ---
    fg_score_up = cv2.resize(fg_score_mf, test_image.size, interpolation=cv2.INTER_CUBIC)

    # --- ì§ì„  + ì¤‘ì‹¬ ê³„ì‚° ---
    overlay_image, mask_bin, centers, detected_height = detect_laser_line(fg_score_up, np.array(test_image))

    # ì¤‘ì‹¬ x ê°„ ê±°ë¦¬ ê³„ì‚°
    dx_text = "N/A"

    # ì—¬ê¸°ì— centersëŠ” 2ì˜ ì´í•˜ì˜ ìˆ˜ì„ -> detect_laser_line í•¨ìˆ˜ì—ì„œ ì œì¼ ìŠ¤ì½”ì–´ ê°’ ë†’ì€ 2ê°œê¹Œì§€ë§Œ ì„ ì •í•´ì™”ìŒ
    if (len(centers) >= 2) or (len(centers) == 1 and detected_height > h * max_height_ratio):
        # ì¤‘ì‹¬ì  ì„ íƒ
        if len(centers) >= 2:
            cx1, cy1 = centers[0]
            cx2, cy2 = centers[1]
        else:  # len == 1
            # í•œê°œë§Œ ê°ì§€ë˜ì—ˆì„ë•ŒëŠ” ë™ì¼í•œ ì„ ìƒì— ë†“ì—¬ì§„ë‹¤ê³  ê°€ì •-> ì¶”í›„ ê´œì°®ì€ì§€ ë´ì•¼í•¨.
            cx1, cy1 = centers[0]
            cx2, cy2 = centers[0]

        # yê°€ ë” í° ì (ì•„ë˜ìª½ ì )ê³¼ ì‘ì€ ì (ìœ„ìª½ ì ) ì°¾ê¸°
        if cy1 > cy2:
            lower_x, lower_y = cx1, cy1
            upper_x, upper_y = cx2, cy2
        else:
            lower_x, lower_y = cx2, cy2
            upper_x, upper_y = cx1, cy1

        dx = lower_x - upper_x  # yê°€ í° ì ì˜ x - yê°€ ì‘ì€ ì ì˜ x
        dx_text = f"{int(round(dx))} px"

        # ì½˜ì†” ì¶œë ¥
        print(f"ğŸ‘‰ upper_x_coordinate: {round(upper_x)}, lower_x_coordinate: {round(lower_x)}")
        print(f"âœ… {file_name} | time: {time.time() - start_time:.3f}s")

        if plot_result:
            visualize_detection_result(file_name=file_name, test_image=test_image, fg_score=fg_score, fg_score_mf=fg_score_mf, overlay_image=overlay_image.copy(), upper_x=upper_x, upper_y=upper_y, lower_x=lower_x, lower_y=lower_y, cx1=cx1, cy1=cy1, cx2=cx2, cy2=cy2, dx_text=dx_text, rotate_angle=rotate_angle)

        return (upper_x, lower_x)

    else:
        print(f"ê°ì§€ëœ laserê°€ 2ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤.")
        return (-1, -1)

# ==========================
# ğŸ”¹ ìœ í‹¸ í•¨ìˆ˜
# ==========================
def resize_transform(img: Image, image_size: int = 768, patch_size: int = CFG.PATCH_SIZE) -> torch.Tensor:
    w, h = img.size
    h_patches = int(image_size / patch_size)
    w_patches = int((w * image_size) / (h * patch_size))
    return TF.to_tensor(TF.resize(img, (h_patches * patch_size, w_patches * patch_size)))

# ==========================
# ğŸ”¹ ì§ì„  ê²€ì¶œ í•¨ìˆ˜
# ==========================
def detect_laser_line(fg_score_mf: np.ndarray, original_image: np.ndarray, threshold=0.6, alpha=0.9, min_area=500):
    """
    fg_score_mf: median filter ì´í›„ foreground heatmap (ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œ ëœ ê²ƒ)
    original_image: RGB numpy array (H, W, 3)
    alpha: ì§ì„ /ë°•ìŠ¤ íˆ¬ëª…ë„
    min_area: ë„ˆë¬´ ì‘ì€ ë…¸ì´ì¦ˆ ì»´í¬ë„ŒíŠ¸ ì œê±°ìš© ìµœì†Œ ë©´ì 
    """
    h = 0
    # 1) thresholdingìœ¼ë¡œ ë¹” ì˜ì—­ë§Œ ì¶”ì¶œ
    mask = (fg_score_mf > threshold).astype(np.uint8) * 255

    # 2) morphologyë¡œ ë…¸ì´ì¦ˆ ì œê±° ë° ì„  ê°•ì¡°
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 9))
    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)

    # 3) connected componentsë¡œ ë¹” ë©ì–´ë¦¬ ë¶„ë¦¬
    num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask_clean, connectivity=8)

    overlay = original_image.copy()
    line_layer = np.zeros_like(overlay)

    centers = []  # (cx, cy) ë¦¬ìŠ¤íŠ¸


    # ğŸ”¹ area ê¸°ì¤€ ìƒìœ„ 2ê°œ ì„ íƒ
    # ğŸ”¹ y ì¤‘ì‹¬ ê¸°ì¤€ upper 1ê°œ + lower 1ê°œ ì„ íƒ (ë©´ì  top-2 ëŒ€ì‹ )
    valid_labels = []
    for label in range(1, num_labels):
        area = stats[label, cv2.CC_STAT_AREA]
        if area < min_area:
            continue
        valid_labels.append(label)

    if len(valid_labels) == 0:
        return blended, mask_clean, centers, h

    # ê° ë¼ë²¨ì˜ y ì¤‘ì‹¬ ê³„ì‚°
    label_y_centers = []
    for label in valid_labels:
        x0, y0, w0, h0 = stats[label, cv2.CC_STAT_LEFT], stats[label, cv2.CC_STAT_TOP], \
                         stats[label, cv2.CC_STAT_WIDTH], stats[label, cv2.CC_STAT_HEIGHT]
        y_center = y0 + h0 / 2
        label_y_centers.append((label, y_center))

    # y ê¸°ì¤€ ì •ë ¬ (ìœ„ìª½ â†’ ì•„ë˜ìª½)
    label_y_centers.sort(key=lambda x: x[1])

    # upper í•˜ë‚˜, lower í•˜ë‚˜ë§Œ ì„ íƒ
    if len(label_y_centers) >= 2:
        selected_labels = [label_y_centers[0][0], label_y_centers[-1][0]]
    else:
        selected_labels = [label_y_centers[0][0]]

    # --- ì„ íƒëœ ë¼ë²¨ë§Œ ì„  ê³„ì‚° ---
    for label in selected_labels:
        component_mask = (labels == label)
        ys, xs = np.where(component_mask)
        weights = fg_score_mf[component_mask].astype(np.float64)

        if weights.sum() == 0:
            continue

        cx = (xs * weights).sum() / weights.sum()
        cy = (ys * weights).sum() / weights.sum()
        centers.append((cx, cy))

        x, y, w, h = stats[label, cv2.CC_STAT_LEFT], stats[label, cv2.CC_STAT_TOP], \
                    stats[label, cv2.CC_STAT_WIDTH], stats[label, cv2.CC_STAT_HEIGHT]
        cv2.rectangle(line_layer, (x, y), (x + w, y + h), (0, 255, 255), 2)
        x_int = int(round(cx))
        cv2.line(line_layer, (x_int, y), (x_int, y + h), (255, 0, 0), 2)

    # íˆ¬ëª…ë„ ì ìš©
    blended = cv2.addWeighted(overlay, 1.0, line_layer, alpha, 0)

    # x ì¢Œí‘œ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
    centers.sort(key=lambda c: c[0])

    return blended, mask_clean, centers, h


def visualize_detection_result(file_name: str, test_image, fg_score, fg_score_mf, overlay_image, upper_x, upper_y, lower_x, lower_y, cx1, cy1, cx2, cy2, dx_text: str, rotate_angle: float = 0.0):
    """
    ğŸ” DINOv3 ì¶”ë¡  ê²°ê³¼ ì‹œê°í™” í•¨ìˆ˜
    """
    # --- ë‘ ì  ì‹œê°í™” (ì„  + ì  + Î”x í…ìŠ¤íŠ¸) ---
    cv2.putText(overlay_image, dx_text, (int((lower_x + upper_x) / 2), int((lower_y + upper_y) / 2)), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2, cv2.LINE_AA)

    cv2.circle(overlay_image, (int(cx1), int(cy1)), 5, (255, 0, 0), -1) # ì²« ë²ˆì§¸ ì 
    cv2.circle(overlay_image, (int(cx2), int(cy2)), 5, (0, 0, 255), -1) # ë‘ ë²ˆì§¸ ì 
    cv2.line(overlay_image, (int(cx1), int(cy1)), (int(cx2), int(cy2)), (0, 255, 0), 2)


    # --- ì‹œê°í™” ---
    plt.figure(figsize=(12, 4), dpi=300)
    plt.suptitle(f"{file_name} (rotate {rotate_angle}Â°)  |  Î”x={dx_text}")

    plt.subplot(1, 4, 1)
    plt.imshow(test_image)
    plt.axis("off")
    plt.title("Rotated Input")

    plt.subplot(1, 4, 2)
    plt.imshow(fg_score, cmap="inferno")
    plt.axis("off")
    plt.title("Foreground Score")

    plt.subplot(1, 4, 3)
    plt.imshow(fg_score_mf, cmap="inferno")
    plt.axis("off")
    plt.title("After Median Filter")

    plt.subplot(1, 4, 4)
    plt.imshow(overlay_image)
    plt.axis("off")
    plt.title("Detected Line + Î”x")

    plt.tight_layout()
    plt.show()


if __name__ == "__main__":
    main()

# ì´ëŸ°ì‹ìœ¼ë¡œ ì¶”ë¡  ìš”ì²­í•´ì•¼í•¨ infer ë¡œ
# var req = new {
#     cmd = "infer",  // ğŸ‘‰ "ì¶”ë¡ ì„ í•´ë‹¬ë¼"ëŠ” ëª…ë ¹ -> ë‚˜ì¤‘ì— infer_top_laser, infer_bottom_laserë¡œ í™•ì¥ê°€ëŠ¥
#     image_data = Convert.ToBase64String(File.ReadAllBytes(imagePath)),
#     rotate_angle = 1.3
# };
# socket.Send(JsonConvert.SerializeObject(req));
