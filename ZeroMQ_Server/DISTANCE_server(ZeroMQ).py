import sys
import pickle
import torch
import io
import time

import os, zmq, json, cv2, numpy as np, base64, datetime, traceback
import torchvision.transforms.functional as TF
import matplotlib.pyplot as plt

from scipy import signal
from PIL import Image

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))


DEBUG_MODE = True
LOG_DIR = "logs"
PORT = 55552

M_cache = None

# âœ… í•œê¸€ í°íŠ¸ ì„¤ì • (Windows ê¸°ì¤€)
plt.rcParams['font.family'] = 'Malgun Gothic'
# âœ… ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['axes.unicode_minus'] = False



# ==========================
# ğŸ”§ ì„¤ì •
# ==========================
class CFG:
    folder_path = "D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3"
    model_path = "D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3/ibakoreaSystem/segmentation/laser_segmentation/weights/polygon_classifier_20251127.pkl"
    test_dir = rf"C:/Users/ë ˆë…¸ë²„/Downloads/Backup_í˜„ì¥/Backup_20250716/TOP_20250716142904" # ë‹¨ì°¨ ì°¨ì´ ì¢€ ë‚˜ëŠ”ê±°

    # ì „ì²˜ë¦¬ (ì¤‘ê°„ ë¶€ë¶„ ì œê±°)
    REMOVE_RATIO = 0.2

    PATCH_SIZE = 8
    IMAGENET_MEAN = (0.485, 0.456, 0.406)
    IMAGENET_STD = (0.229, 0.224, 0.225)
    WEIGHTS_PATH = "D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3/dinov3_vits16_pretrain_lvd1689m-08c60483.pth"

    MODEL_TO_NUM_LAYERS = {"dinov3_vits16": 12, "dinov3_vits16plus": 12, "dinov3_vitb16": 12, "dinov3_vitl16": 24, "dinov3_vith16plus": 32, "dinov3_vit7b16": 40}
    MODEL_NAME = "dinov3_vits16"
    n_layers = MODEL_TO_NUM_LAYERS[MODEL_NAME]
    IMAGE_SIZE = 768
    min_height = 60 # TOP ì—ì„œëŠ” 80ì´ ë§ê³ , BOTTOMì—ì„œëŠ” width ë¥¼ ì¡°ì ˆí•˜ëŠ”ê²Œ ë‚˜ì„ê±° ê°™ìŒ
    confidence = 0.7 # activationì´ í•´ë‹¹ê°’ ì´ìƒì¼ë•Œë§Œ íƒì§€ ì¤‘
    rotate_angle = 4  # TOPì€ 1.3ë„ ê°€ ì œì¼ ì ì ˆí•˜ê³  , BOTTOMì€ 4ë„ widthê°€ ë„ˆë¬´ ì‘ìœ¼ë©´ ã…‡ã…‡ ì•ˆí•˜ê²Œë” (widthëŠ” ì¡°ì •í•´ë³¼ê²ƒ)
    plot_result = True if DEBUG_MODE else False
    bbox_center = True # True ì‹œ bounding boxì˜ ì • ì¤‘ì•™ì„ ì°¾ìŒ, Falseì‹œ activationì˜ ê°€ì¤‘ í‰ê· ê°’ì„ ì°¾ìŒ
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    
def write_log(message: str):
    # DEBUG ëª¨ë“œì˜ ê²½ìš° printë¡œë„ ë„ì›€
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    os.makedirs(LOG_DIR, exist_ok=True)

    log_path = os.path.join(LOG_DIR, f"{today}_laser.log")

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{timestamp}] {message}"

    with open(log_path, "a", encoding="utf-8") as f:
        f.write(line + "\n")

    if DEBUG_MODE:
        print(line)
        
    # log íŒŒì¼ ì‚­ì œëŠ” ok_ngì—ì„œ ì§„í–‰í•¨ - 365ì¼ë§Œ ë‚¨ê¹€


def decode_base64_image(img_b64):
    try:
        img_bytes = base64.b64decode(img_b64)
        img = Image.open(io.BytesIO(img_bytes)).convert("RGB")
        return img
    except Exception as e:
        write_log(f"âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None


def model_load():
    with open(CFG.model_path, "rb") as f:
        clf = pickle.load(f)

    model = torch.hub.load(repo_or_dir=CFG.folder_path, model=CFG.MODEL_NAME, source="local", weights=CFG.WEIGHTS_PATH)
    use_fp16 = torch.cuda.is_available()
    model = model.eval().to(CFG.device)
    if use_fp16:
        model = model.half()
        write_log("ğŸš€ GPU + FP16 ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
    else:
        write_log("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (FP32)")

    try:
        dummy = torch.randn(1, 3, CFG.IMAGE_SIZE, CFG.IMAGE_SIZE, device=CFG.device)
        if use_fp16:
            dummy = dummy.half()
        with torch.inference_mode():
            _ = model(dummy)
        write_log("ğŸ”¥ ëª¨ë¸ warm-up ì™„ë£Œ")
    except Exception as e:
        write_log(f"âš ï¸ warm-up ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    return model, clf, CFG.device, use_fp16

# ëª…ì•” êµ¬ë¶„í•´ì„œ ë°ì€ê³³ ë” ë°ê²Œ ì–´ë‘ìš´ ê³³ ë” ì–´ë‘¡ê²Œ
def s_curve(img, strength=0.5):
    """
    strength = 0.0 ~ 1.0 (0.5 ì¶”ì²œ)
    S-curve: ë°ì€ê³³â†‘ ì–´ë‘ìš´ê³³â†“
    """
    img = img.astype(np.float32) / 255.0

    # S-curve
    out = img + strength * (img - img**2)  # S ì»¤ë¸Œ
    out = np.clip(out * 255, 0, 255).astype(np.uint8)
    return out


def main():
    model, clf, device, use_fp16 = model_load()
    context = zmq.Context()
    socket = context.socket(zmq.REP)
    socket.bind(f"tcp://*:{PORT}")

    write_log(f"âœ… ZMQ ì„œë²„ ì‹¤í–‰ë¨ (í¬íŠ¸ {PORT})")

    while True:
        try:
            # ğŸ”¥ 1) ë©€í‹°í”„ë ˆì„ ìˆ˜ì‹ 
            frames = socket.recv_multipart()

            # [Frame 1] = file name (string)
            file_name = frames[0].decode()

            # [Frame 2] = image binary
            img_bytes = frames[1]

            write_log(f"ğŸ“© ìš”ì²­ ìˆ˜ì‹ : file={file_name}")

            # ğŸ”¥ 2) ì´ë¯¸ì§€ ë””ì½”ë”©
            np_arr = np.frombuffer(img_bytes, dtype=np.uint8)
            image_np = cv2.imdecode(np_arr, cv2.IMREAD_COLOR)

            # # ğŸ”¥ 2.5) ì´ë¯¸ì§€ S curve -> í•´ë„ íš¨ê³¼ ë¯¸ë¯¸í•¨
            # image_np = s_curve(image_np, strength=0.6)

            if image_np is None:
                raise ValueError("ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨")

            # ğŸ”¥ 3) ì¶”ë¡  ìˆ˜í–‰
            result = infer_image_one(
                model, clf, device, use_fp16,
                image_np,
                rotate_angle=CFG.rotate_angle,
                plot_result=CFG.plot_result
            )

            # ğŸ”¥ 4) ê²°ê³¼ ì „ì†¡(JSONë§Œ ìœ ì§€)
            if isinstance(result, tuple):
                socket.send_string(json.dumps({
                    "status": "ê²€ì¶œ ì™„ë£Œ",
                    "upper_x": round(result[0]),
                    "lower_x": round(result[1])
                }))
            else:
                socket.send_string(json.dumps({
                    "status": "ê²€ì¶œ ì‹¤íŒ¨",
                    "upper_x": -1,
                    "lower_x": -1
                }))

        except Exception as e:
            err_msg = f"Error: {type(e).__name__}: {str(e)}"
            write_log(err_msg)
            traceback.print_exc()
            socket.send_string(json.dumps({"status": "ERROR", "msg": err_msg}))

# ==========================
# ğŸ”¹ ì´ë¯¸ì§€ í´ë” ì¶”ë¡  (ì§ì„  ê²€ì¶œ í¬í•¨)
# ==========================
def infer_image_one(model, clf, device, use_fp16, test_image,
                    rotate_angle: float = 1.3, plot_result: bool = False):

    start_time = time.time()

    # --- íšŒì „ ì ìš© ---
    img_np = np.array(test_image)
    h, w = img_np.shape[:2]

    center = (w // 2, h // 2)
    M = cv2.getRotationMatrix2D(center, rotate_angle, 1.0)
    rotated = cv2.warpAffine(img_np, M, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_REPLICATE)

    # --- ìƒ/í•˜ ë¶„ë¦¬ ---
    upper_img = rotated[:h // 2, :]
    lower_img = rotated[h // 2:, :]

    # --- upper / lower ê°ê° ë ˆì´ì € 1ê°œ ê²€ì¶œ ---
    upper_result = detect_single_laser(model, clf, device, use_fp16, upper_img, min_height=CFG.min_height)
    lower_result = detect_single_laser(model, clf, device, use_fp16, lower_img, min_height=CFG.min_height)


    upper_x, upper_vis = upper_result  # (xì¢Œí‘œ, ì‹œê°í™” ë„˜íŒŒì´)
    lower_x, lower_vis = lower_result

    # --- Î”x ê³„ì‚° ---
    if upper_x < 0 or lower_x < 0:
        write_log("âŒ ë ˆì´ì € ê²€ì¶œ ì‹¤íŒ¨ \n")
        return (-1, -1)

    dx = lower_x - upper_x
    dx_text = f"{int(dx)} px"

    write_log(f"ğŸ‘‰ upper_x: {upper_x}, lower_x: {lower_x}, Î”x={dx_text}")
    write_log(f"â± time: {time.time() - start_time:.3f}s")

    # --- ì‹œê°í™” ---
    if plot_result:
        visualize_upper_lower(rotated, upper_vis, lower_vis, upper_x, lower_x, dx_text, rotate_angle)

    return (upper_x, lower_x)

def detect_single_laser(model, clf, device, use_fp16, img_np, min_height=80):
    """
    ì…ë ¥: ìƒë‹¨ or í•˜ë‹¨ ì´ë¯¸ì§€
    ì¶œë ¥: (xì¢Œí‘œ, ì‹œê°í™” ëœ ì´ë¯¸ì§€)

    min_height -> ë†’ì´ê°€ 80px ë³´ë‹¤ ì‘ì„ê²½ìš° ë ˆì´ì € ì„ ì´ ì•„ë‹Œê²ƒìœ¼ë¡œ ê°„ì£¼í•¨ -> BOTTOMì€ ë†’ì´ê°€ ì‘ì€ê²ƒë„ ì€ê·¼ ë§ë„¤ìš”
    """

    h, w = img_np.shape[:2]
    pil_img = Image.fromarray(img_np)

    # --- DINO ì „ì²˜ë¦¬ ---
    img_resized = resize_transform(pil_img)
    img_tensor = TF.normalize(img_resized, mean=CFG.IMAGENET_MEAN, std=CFG.IMAGENET_STD)
    img_tensor = img_tensor.unsqueeze(0).to(device)

    if use_fp16:
        img_tensor = img_tensor.half()

    # --- DINO feature ---
    with torch.inference_mode():
        feats = model.get_intermediate_layers(
            img_tensor, n=range(CFG.n_layers), reshape=True, norm=True
        )
        x = feats[-1].squeeze().detach().cpu()
        dim, H_feat, W_feat = x.shape
        x = x.view(dim, -1).permute(1, 0)

    # --- classifier ---
    fg_score = clf.predict_proba(x)[:, 1]
    fg_score = fg_score.reshape(H_feat, W_feat)
    fg_score_mf = signal.medfilt2d(fg_score, kernel_size=3)

    # --- ì›ë³¸ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œ ---
    fg_up = cv2.resize(fg_score_mf, (w, h), interpolation=cv2.INTER_CUBIC)

    # --- threshold ---
    mask = (fg_up > CFG.confidence).astype(np.uint8) * 255
    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3, 9))
    mask_clean = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)

    # --- connected components ---
    num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(mask_clean)

    # ê°€ì¥ í° area 1ê°œë§Œ ì„ íƒ
    best_label = -1
    best_area = 0
    for label in range(1, num_labels):
        area = stats[label, cv2.CC_STAT_AREA]
        if area > best_area:
            best_area = area
            best_label = label

    if best_label == -1:
        return -1, img_np  # ì‹¤íŒ¨

    # --- ì‹œê°í™” ---
    vis = img_np.copy()
    x0, y0, w0, h0 = stats[best_label, cv2.CC_STAT_LEFT], stats[best_label, cv2.CC_STAT_TOP], \
                     stats[best_label, cv2.CC_STAT_WIDTH], stats[best_label, cv2.CC_STAT_HEIGHT]

    # --- ì¤‘ì‹¬ êµ¬í•˜ê¸° ---
    if CFG.bbox_center:
        # --- ì¤‘ì‹¬ êµ¬í•˜ê¸°: Bounding Boxì˜ ê¸°í•˜í•™ì  ì¤‘ì•™ ---
        cx = x0 + w0 / 2
        cy = y0 + h0 / 2
    else:
        ys, xs = np.where(labels == best_label)
        weights = fg_up[labels == best_label].astype(float)

        cx = (xs * weights).sum() / weights.sum()
        cy = (ys * weights).sum() / weights.sum()

    if h0 < min_height:
        # ì´ê²Œ ì§§ìœ¼ë©´ ì˜¤íƒì´ ëœ¨ê³ , ê¸¸ë©´ ì‹¤ì œ íƒì§€ë˜ì•¼ í•  ê²ƒ ë„ ì•ˆë¨.
        write_log(f'íƒì§€ëœ ë°•ìŠ¤ì˜ ë†’ì´ê°€ ìµœì†Œ ë†’ì´ë³´ë‹¤ ì‘ì€ ìƒíƒœì…ë‹ˆë‹¤. ì˜ˆì¸¡ ë†’ì´: {h0} / ìµœì†Œ ë†’ì´: {min_height}')
        return -1, img_np

    cv2.rectangle(vis, (x0, y0), (x0 + w0, y0 + h0), (0, 255, 255), 2)
    cv2.line(vis, (int(cx), y0), (int(cx), y0 + h0), (0, 0, 255), 2)
    cv2.circle(vis, (int(cx), int(cy)), 5, (0, 0, 255), -1)

    # ============================
    # ğŸ”¥ DEBUG ëª¨ë“œì¼ ë•Œë§Œ heatmap ê³„ì‚°
    # ============================
    if CFG.plot_result:
        heatmap = (fg_up * 255).astype(np.uint8)
        heatmap_color = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)

        # ì›ë³¸ vis + heatmap overlay
        overlay = cv2.addWeighted(vis, 0.85, heatmap_color, 0.15, 0)

        return round(cx), overlay

    # DEBUG off â†’ heatmap ì—†ì´ ê¸°ë³¸ bboxë§Œ ë°˜í™˜
    return round(cx), vis


# ==========================
# ğŸ”¹ ìœ í‹¸ í•¨ìˆ˜
# ==========================
def resize_transform(img: Image, image_size: int = 768, patch_size: int = CFG.PATCH_SIZE) -> torch.Tensor:
    w, h = img.size
    h_patches = int(image_size / patch_size)
    w_patches = int((w * image_size) / (h * patch_size))
    return TF.to_tensor(TF.resize(img, (h_patches * patch_size, w_patches * patch_size)))


def visualize_upper_lower(rotated, upper_vis, lower_vis, upper_x, lower_x, dx_text, rotate_angle):
    h, w = rotated.shape[:2]

    merged_overlay = rotated.copy()
    merged_overlay[:h//2, :] = upper_vis
    merged_overlay[h//2:, :] = lower_vis

    # RGB ë³€í™˜
    merged_overlay = cv2.cvtColor(merged_overlay, cv2.COLOR_BGR2RGB)
    rotated_rgb = cv2.cvtColor(rotated, cv2.COLOR_BGR2RGB)

    import matplotlib
    matplotlib.use("TkAgg")

    fig = plt.figure(figsize=(12, 6))

    # ğŸ”¥ ì°½ ìœ„ì¹˜ ê³ ì • : ê°€ë¡œ ì¤‘ì•™ + ì„¸ë¡œ ìµœìƒë‹¨
    manager = plt.get_current_fig_manager()
    window = manager.window
    window.update_idletasks()

    x = 300
    y = 0
    window.geometry(f"+{x}+{y}")

    # --- subplot
    ax1 = fig.add_subplot(1, 2, 1)
    ax1.imshow(rotated_rgb)
    ax1.set_title(f"â‘  íšŒì „ëœ ì´ë¯¸ì§€ (rotate={rotate_angle}Â°)")
    ax1.axis("off")

    ax2 = fig.add_subplot(1, 2, 2)
    ax2.imshow(merged_overlay)
    ax2.set_title(f"â‘¡ ë ˆì´ì € ê²€ì¶œ Overlay (Î”x={dx_text})")
    ax2.axis("off")

    # --- Colorbar ---
    cmap = matplotlib.colormaps.get_cmap("jet")
    norm = matplotlib.colors.Normalize(vmin=0.0, vmax=1.0)

    cax = fig.add_axes([0.92, 0.15, 0.02, 0.7])
    cb = plt.colorbar(matplotlib.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cax)
    cb.set_label("Activation (0 ~ 1)")

    cb.ax.hlines(f'{CFG.confidence}', 0, 1, colors="black", linewidth=2)
    cb.ax.text(1.1, CFG.confidence, f'{CFG.confidence}', color="black", va="center", fontsize=8)

    plt.show(block=True)


if __name__ == "__main__":
    main()