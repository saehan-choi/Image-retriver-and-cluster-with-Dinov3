import os, zmq, json, cv2, numpy as np, base64, datetime, traceback
import torch

import albumentations as A

from torch import nn
from albumentations.pytorch import ToTensorV2
from concurrent.futures import ThreadPoolExecutor

# ==========================
# âš™ï¸ ì„¤ì •
# ==========================
DEBUG_MODE = True
LOG_DIR = "logs"
PORT = 55551


# ==========================
# ğŸ§¾ ë¡œê·¸ í•¨ìˆ˜
# ==========================
def write_log(message: str):
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    os.makedirs(LOG_DIR, exist_ok=True)
    log_path = os.path.join(LOG_DIR, f"{today}_okng.log")

    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    line = f"[{timestamp}] {message}"

    with open(log_path, "a", encoding="utf-8") as f:
        f.write(line + "\n")

    if DEBUG_MODE:
        print(line)

    cleanup_logs(keep_last_n=365) # ìµœê·¼ 365ê°œ íŒŒì¼ë§Œ ë‚¨ê¸°ê³  ë‹¤ë¥¸ê±´ ì‚­ì œ

def cleanup_logs(keep_last_n=5):
    """
    LOG_DIR ì•ˆì˜ ë¡œê·¸ íŒŒì¼ ì¤‘ ìµœì‹  Nê°œë§Œ ë‚¨ê¸°ê³  ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
    ì‚­ì œ ë‚´ìš©ë„ ì˜¤ëŠ˜ ë¡œê·¸ íŒŒì¼ì— ì•ˆì „í•˜ê²Œ ê¸°ë¡ (ë¬´í•œë£¨í”„ ì—†ìŒ)
    """
    if not os.path.exists(LOG_DIR):
        return

    log_files = sorted(
        [os.path.join(LOG_DIR, f) for f in os.listdir(LOG_DIR) if f.endswith(".log")],
        key=os.path.getmtime
    )

    if len(log_files) <= keep_last_n:
        return

    files_to_delete = log_files[:-keep_last_n]

    # ì˜¤ëŠ˜ ë¡œê·¸ íŒŒì¼ì— ì‚­ì œ ê¸°ë¡ ë‚¨ê²¨ì•¼ í•˜ë‹ˆê¹Œ ê²½ë¡œ ì¤€ë¹„
    today = datetime.datetime.now().strftime("%Y-%m-%d")
    today_log_path = os.path.join(LOG_DIR, f"{today}_okng.log")

    for f in files_to_delete:
        try:
            os.remove(f)

            # ğŸ”¥ write_log() í˜¸ì¶œ ê¸ˆì§€ â†’ ë¬´í•œë£¨í”„ ë§‰ê¸°
            # ëŒ€ì‹  ì§ì ‘ íŒŒì¼ì— í•œ ì¤„ë§Œ append
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            line = f"[{timestamp}] ğŸ—‘ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ: {os.path.basename(f)}\n"

            with open(today_log_path, "a", encoding="utf-8") as logf:
                logf.write(line)

            if DEBUG_MODE:
                print(line.strip())

        except Exception as e:
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            line = f"[{timestamp}] âŒ ë¡œê·¸ ì‚­ì œ ì‹¤íŒ¨: {e}\n"

            with open(today_log_path, "a", encoding="utf-8") as logf:
                logf.write(line)

            if DEBUG_MODE:
                print(line.strip())

# ==========================
# ğŸ§  ëª¨ë¸ ì„¤ì •
# ==========================
class CFG:
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dinov3_location = rf"D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3/"
    model_name = "dinov3_vits16"
    dinov3_weights_path = rf"D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3/dinov3_vits16_pretrain_lvd1689m-08c60483.pth"
    weights_path = rf"D:\iba\POSCO_welding(2025.09.30~2025.12.30)\dinov3\ibakoreaSystem\classification\weights\DINOv3_linear_best_epoch50.pth"
    in_features = 384
    model_num_class = 2
    batch_size = 8   # âœ… ê¸°ë³¸ ë°°ì¹˜ í¬ê¸° (C#ì—ì„œ ë®ì–´ì“¸ ìˆ˜ë„ ìˆìŒ)
    img_resize = (418, 418)
    skip_img_cnt_head = 4
    skip_img_cnt_tail = 4
    use_fp16 = True


# ==========================
# ğŸ§  DINOv3 ëª¨ë¸
# ==========================
class DinoLinearClassifier(nn.Module):
    def __init__(self, num_classes=2):
        super().__init__()
        self.model = torch.hub.load(
            repo_or_dir=CFG.dinov3_location,
            model=CFG.model_name,
            source="local",
            weights=CFG.dinov3_weights_path,
        )
        self.fc = nn.Linear(CFG.in_features, num_classes)

    def forward(self, x):
        with torch.no_grad():
            outputs = self.model.forward_features(x)
            if isinstance(outputs, dict):
                x = outputs.get("x_norm_clstoken", list(outputs.values())[0])
            else:
                x = outputs
        return self.fc(x)


# ==========================
# ğŸš€ ëª¨ë¸ ì´ˆê¸°í™”
# ==========================
model = None

def init_model():
    global model
    try:
        model = DinoLinearClassifier(num_classes=CFG.model_num_class).to(CFG.device)
        state_dict = torch.load(CFG.weights_path, map_location=CFG.device)
        fixed_state_dict = {k.replace("backbone.", "model."): v for k, v in state_dict.items()}
        model.load_state_dict(fixed_state_dict, strict=False)
        model.eval()

        if CFG.use_fp16 and CFG.device == "cuda":
            model.half()
            write_log("ğŸš€ GPU + FP16 ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘")
        else:
            write_log("ğŸ’» CPU ëª¨ë“œë¡œ ì‹¤í–‰ ì¤‘ (FP32)")

        # ë”ë¯¸ í…ŒìŠ¤íŠ¸
        dummy = torch.randn(1, 3, *CFG.img_resize).to(CFG.device)
        if CFG.use_fp16 and CFG.device == "cuda":
            dummy = dummy.half()
        outputs = model(dummy)
        write_log("ğŸ”¥ ëª¨ë¸ warm-up ì™„ë£Œ")
        return True
    except Exception as e:
        write_log(f"âŒ ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        traceback.print_exc()
        return False


# ==========================
# ğŸ§© ì´ë¯¸ì§€ ë³€í™˜ í•¨ìˆ˜
# ==========================
def decode_base64_image(img_b64):
    try:
        img_bytes = base64.b64decode(img_b64)
        img_np = np.frombuffer(img_bytes, np.uint8)
        return cv2.imdecode(img_np, cv2.IMREAD_COLOR)
    except Exception as e:
        write_log(f"âŒ ì´ë¯¸ì§€ ë””ì½”ë”© ì‹¤íŒ¨: {e}")
        return None


# ==========================
# ğŸ”® ë°°ì¹˜ ì¶”ë¡ 
# ==========================
def execute_inference_batch(frames):
    try:

        write_log(f"ğŸ“¦ Batch ìš”ì²­ ìˆ˜ì‹ : {len(frames)}ì¥")

        images = []
        for raw_bytes in frames:
            img_np = np.frombuffer(raw_bytes, np.uint8)
            img = cv2.imdecode(img_np, cv2.IMREAD_COLOR)
            if img is not None:
                images.append(img)

        transform = A.Compose([
            A.Resize(*CFG.img_resize),
            A.Normalize(),
            ToTensorV2(),
        ])

        # ğŸ”¹ ThreadPoolExecutorë¡œ ë³‘ë ¬ ë³€í™˜
        with ThreadPoolExecutor(max_workers=8) as executor:
            tensors = list(executor.map(
                lambda img: transform(
                    image=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                )["image"],
                images
            ))

        tensors = [t for t in tensors if t is not None]
        if not tensors:
            raise ValueError("ìœ íš¨í•œ ì´ë¯¸ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.")

        # ğŸ”¹ ë°°ì¹˜ í…ì„œë¡œ í•©ì¹˜ê¸°
        batch_tensor = torch.stack(tensors).to(CFG.device)
        if CFG.use_fp16 and CFG.device == "cuda":
            batch_tensor = batch_tensor.half()
            model.half()

        # ğŸ”¹ ì¶”ë¡ 
        model.eval()
        with torch.no_grad():
            outputs = model(batch_tensor)
            probs = torch.softmax(outputs, dim=1)
            pred_classes = torch.argmax(probs, dim=1).cpu().numpy()
            confidences = probs.max(dim=1).values.cpu().numpy()

        # ğŸ”¹ ê²°ê³¼ ìƒì„±
        results = []
        for idx, (cls, conf) in enumerate(zip(pred_classes, confidences)):
            label = "OK" if cls == 0 else "NG"
            results.append({
                "index": idx,
                "result": label,
                "confidence": round(float(conf), 3),
                "class_id": int(cls)
            })

        write_log(f"âœ… Batch ì¶”ë¡  ì™„ë£Œ â€” {len(results)}ê°œ ì²˜ë¦¬ë¨.")
        return results

    except Exception as e:
        write_log(f"âŒ Batch ì¶”ë¡  ì¤‘ ì˜¤ë¥˜: {e}")
        traceback.print_exc()
        return [{"status": "ERROR", "msg": str(e)}]


# ==========================
# ğŸŒ ZMQ ì„œë²„
# ==========================
def run_server():
    context = zmq.Context()
    socket = context.socket(zmq.REP)
    socket.bind(f"tcp://*:{PORT}")
    init_model()

    write_log(f"âœ… ZMQ ì„œë²„ ì‹¤í–‰ë¨ (í¬íŠ¸ {PORT})")

    while True:
        try:
            # âœ… ì—¬ëŸ¬ í”„ë ˆì„(ì´ë¯¸ì§€) ìˆ˜ì‹  
            frames = socket.recv_multipart()  # ì—¬ëŸ¬ í”„ë ˆì„ ë°›ê¸°
            write_log(f"ğŸ“© ë°›ì€ í”„ë ˆì„ ìˆ˜: {len(frames)}")

            # ìš”ì²­ íƒ€ì…ì— ë”°ë¼ ë‹¨ì¼ / ë°°ì¹˜ êµ¬ë¶„
            result = execute_inference_batch(frames)

            socket.send_string(json.dumps(result, ensure_ascii=False))

        except Exception as e:
            err_msg = f"âŒ ì„œë²„ ì˜¤ë¥˜: {e}"
            write_log(err_msg)
            socket.send_string(json.dumps({"status": "ERROR", "msg": str(e)}))
            traceback.print_exc()


# ==========================
# ğŸ§© ì‹¤í–‰
# ==========================
if __name__ == "__main__":
    run_server()

